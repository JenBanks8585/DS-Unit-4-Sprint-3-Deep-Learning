{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-MNA-DS11",
      "language": "python",
      "name": "u4-s3-mna-ds11"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.23.3"
    },
    "colab": {
      "name": "Jen_Banks_assignment LS_DS_431_RNN_and_LSTM.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkxCLlF9EUfH",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjiMwrs5GWT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "95eec702-2ff4-4973-9d78-dc0de3485fa0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.94.6.146:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.94.6.146:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.94.6.146:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.94.6.146:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgu_K3CiGlGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4e1fd099-3b10-463b-a0fe-0fb8ad5ac0c3"
      },
      "source": [
        "try:\n",
        "  device_name = os.environ[\"COLAB_TPU_ADDR\"]\n",
        "  TPU_ADDRESS = \"qrpc://\" + device_name\n",
        "  print(\"Found TPU at : {}\".format(TPU_ADDRESS))\n",
        "except KeyError:\n",
        "  print(\"TPU not found\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at : qrpc://10.94.6.146:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-06-15T18:18:20.442Z",
          "iopub.execute_input": "2020-06-15T18:18:20.453Z",
          "iopub.status.idle": "2020-06-15T18:18:20.513Z",
          "shell.execute_reply": "2020-06-15T18:18:20.523Z"
        },
        "id": "F1iCZYLeEUfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-06-15T18:25:49.778Z",
          "iopub.execute_input": "2020-06-15T18:25:49.781Z",
          "iopub.status.idle": "2020-06-15T18:25:51.467Z",
          "shell.execute_reply": "2020-06-15T18:25:51.469Z"
        },
        "id": "P0ZLH8wxEUfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "\n",
        "r = requests.get(url)\n",
        "r.encoding = r.apparent_encoding\n",
        "data = r.text\n",
        "data = data.split('\\r\\n')\n",
        "toc = [l.strip() for l in data[44:130:2]]\n",
        "# Skip the Table of Contents\n",
        "data = data[135:]\n",
        "\n",
        "# Fixing Titles\n",
        "toc[9] = 'THE LIFE OF KING HENRY V'\n",
        "toc[18] = 'MACBETH'\n",
        "toc[24] = 'OTHELLO, THE MOOR OF VENICE'\n",
        "toc[34] = 'TWELFTH NIGHT: OR, WHAT YOU WILL'\n",
        "\n",
        "locations = {id_:{'title':title, 'start':-99} for id_,title in enumerate(toc)}\n",
        "\n",
        "# Start \n",
        "for e,i in enumerate(data):\n",
        "    for t,title in enumerate(toc):\n",
        "        if title in i:\n",
        "            locations[t].update({'start':e})\n",
        "            \n",
        "\n",
        "df_toc = pd.DataFrame.from_dict(locations, orient='index')\n",
        "df_toc['end'] = df_toc['start'].shift(-1).apply(lambda x: x-1)\n",
        "df_toc.loc[42, 'end'] = len(data)\n",
        "df_toc['end'] = df_toc['end'].astype('int')\n",
        "\n",
        "df_toc['text'] = df_toc.apply(lambda x: '\\r\\n'.join(data[ x['start'] : int(x['end']) ]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "execution": {
          "iopub.status.busy": "2020-06-15T18:26:12.630Z",
          "iopub.execute_input": "2020-06-15T18:26:12.637Z",
          "iopub.status.idle": "2020-06-15T18:26:12.643Z",
          "shell.execute_reply": "2020-06-15T18:26:12.647Z"
        },
        "id": "_Ac4hCPFEUfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "807db842-8232-4e38-a950-3f95d5c9d3b3"
      },
      "source": [
        "#Shakespeare Data Parsed by Play\n",
        "df_toc.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ALL’S WELL THAT ENDS WELL</td>\n",
              "      <td>2777</td>\n",
              "      <td>7738</td>\n",
              "      <td>ALL’S WELL THAT ENDS WELL\\r\\n\\r\\n\\r\\n\\r\\nConte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>THE TRAGEDY OF ANTONY AND CLEOPATRA</td>\n",
              "      <td>7739</td>\n",
              "      <td>11840</td>\n",
              "      <td>THE TRAGEDY OF ANTONY AND CLEOPATRA\\r\\n\\r\\nDRA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AS YOU LIKE IT</td>\n",
              "      <td>11841</td>\n",
              "      <td>14631</td>\n",
              "      <td>AS YOU LIKE IT\\r\\n\\r\\nDRAMATIS PERSONAE.\\r\\n\\r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THE COMEDY OF ERRORS</td>\n",
              "      <td>14632</td>\n",
              "      <td>17832</td>\n",
              "      <td>THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>THE TRAGEDY OF CORIOLANUS</td>\n",
              "      <td>17833</td>\n",
              "      <td>27806</td>\n",
              "      <td>THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 title  ...                                               text\n",
              "0            ALL’S WELL THAT ENDS WELL  ...  ALL’S WELL THAT ENDS WELL\\r\\n\\r\\n\\r\\n\\r\\nConte...\n",
              "1  THE TRAGEDY OF ANTONY AND CLEOPATRA  ...  THE TRAGEDY OF ANTONY AND CLEOPATRA\\r\\n\\r\\nDRA...\n",
              "2                       AS YOU LIKE IT  ...  AS YOU LIKE IT\\r\\n\\r\\nDRAMATIS PERSONAE.\\r\\n\\r...\n",
              "3                 THE COMEDY OF ERRORS  ...  THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...\n",
              "4            THE TRAGEDY OF CORIOLANUS  ...  THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Nx9SHdE-Ew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "6929e07a-0c8b-4983-ec18-6b1e6d4e4f03"
      },
      "source": [
        "df_toc.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 43 entries, 0 to 42\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   title   43 non-null     object\n",
            " 1   start   43 non-null     int64 \n",
            " 2   end     43 non-null     int64 \n",
            " 3   text    43 non-null     object\n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 2.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_3Vr5CeFthI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "35b4dd32-264d-4bd0-c0ed-8596cf53ad71"
      },
      "source": [
        "len(toc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuoR-t-cLF4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Gather all text \n",
        "# Why? 1. See all possible characters 2. For training / splitting later\n",
        "text = \" \".join(df_toc['text'].values)\n",
        "\n",
        "# Unique Characters\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C56dalt6MpZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82da23fa-f46a-4437-fe05-162a9547a925"
      },
      "source": [
        "text[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'L'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ6STD3x4kgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e82de55e-56ce-4ddf-eb3a-5cc63af1da53"
      },
      "source": [
        "toc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ALL’S WELL THAT ENDS WELL'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQfNMqxSroCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4c75fec6-1a7f-42dd-b263-8cc8724f1dbb"
      },
      "source": [
        "len(text), len(chars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5635845, 106)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HFFbHaFMpms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "11627ce8-0472-4248-c443-3491ed70a04e"
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "maxlen = 70  # 40 charcaters\n",
        "step = 20      # choose 40, then step 5, choose another set of 40, step 5 choose set of 40 again\n",
        "\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  760289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESoOv5CPMpyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Create x & y\n",
        "\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5meRlWESMp8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d9bc863c-3419-4c5f-f431-bf37d1ace2fd"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((281789, 70, 106), (281789, 106))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RHWqyChlR9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "630fHKP77Y9X",
        "colab_type": "text"
      },
      "source": [
        "### Attempt 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhfKnLCeMp6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIYzToDaMp3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuyvyNDLMpsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        #sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2RiFl7HrUfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "87e58d88-976d-4ce1-d380-18c9810593a2"
      },
      "source": [
        "x[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       ...,\n",
              "       [False,  True, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0StiCnWMpgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe8a098b-c37b-4add-f11f-3d61f6a0be8f"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 2.2652\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"d overcome us like a summer’s cloud,\n",
            "Without our special wonder? You \"\n",
            "d overcome us like a summer’s cloud,\n",
            "Without our special wonder? You thos wore ture hisild”\n",
            "Lr or sake, you wiys whem olk my ‘in somy,\n",
            "A’d he’d co ham!\n",
            "  RPtAFRTUS.\n",
            "By wholl gepore feruss hass nod the Prore,\n",
            "A on my bufutiand, souls.\n",
            "\n",
            "BLSTIVIOLO.\n",
            "\n",
            "    1Youssaim on hat’s faksicst\n",
            "    Pnath thel a be Jow bleves; times a chacenef\n",
            "    musing boant diy, is and I mes.\n",
            "\n",
            "   Cheve lim; now: nomd pa,twe the there\n",
            "5940/5940 [==============================] - 1392s 234ms/step - loss: 2.2652\n",
            "Epoch 2/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.8921\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"in in him. Thou dost consent\n",
            "    In some large measure to thy father'\"\n",
            "in in him. Thou dost consent\n",
            "    In some large measure to thy father's and dim\n",
            "    Frit lore mint ble’s thou likan ore\n",
            "    Whel and seem anvinct as fase ofr samseecty?\n",
            "    Were onos us lave when, hone to he compe'tes; be mangring sich at you,\n",
            "Lite mary anrenge.                          3veres your reant.\n",
            "  CRAYDUUE. Sot halls on apont me if thy is pendry that Kis prow fligh me,\n",
            "Wentorres agave take ofon what he tome, the poraty;\n",
            "Phipk sroy have be pigpur my \n",
            "5940/5940 [==============================] - 1439s 242ms/step - loss: 1.8921\n",
            "Epoch 3/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.7729\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"at. O, be not like your mistress! Be moved, be moved.\n",
            "               \"\n",
            "at. O, be not like your mistress! Be moved, be moved.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  HEEA. And alls take so he be tatse semple,\n",
            "    Ad your frack his be a my soust fring heave to is;\n",
            "    Weat it leck? Benoug I hads the Dulk?\n",
            "  TRIIA. Mored Of your frake will me, ars a wownt!\n",
            "    And preal’s, my is; a br'et for shmerventuringe\n",
            "    Thous I hives lied your sheall praie\n",
            "                      Enter PAIE, and sir.\n",
            "\n",
            "Spor sir, twene\n",
            "5940/5940 [==============================] - 1454s 245ms/step - loss: 1.7729\n",
            "Epoch 4/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.6992\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"or.\n",
            "    Call hither to the stake my two brave bears,\n",
            "    That with t\"\n",
            "or.\n",
            "    Call hither to the stake my two brave bears,\n",
            "\n",
            "\n",
            "    To ban shomand, as veremence.  ous,\n",
            "    This profelon, soy not where to noth there;\n",
            "    When then but know, and Queet of brong.\n",
            "  DUKINGGRER We lare by dishours, which dost shave stoars lece\n",
            "                            Enter TISTIMON\n",
            "\n",
            "  QUEV\n",
            "\n",
            "  IHTI. He as have in wive thy sunting mojefter.\n",
            "  FRRSCEND\n",
            "5940/5940 [==============================] - 1459s 246ms/step - loss: 1.6992\n",
            "Epoch 5/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.6471\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"riting, fairly drawn.\n",
            "\n",
            "BIANCA.\n",
            "Why, I am past my gamut long ago.\n",
            "\"\n",
            "riting, fairly drawn.\n",
            "\n",
            "BIANCA.\n",
            "Why, I am past my gamut long ago.\n",
            "\n",
            "PALLOD.\n",
            "Count she let we his love\n",
            "Her beine o’er offats but crive, subvortes me,\n",
            "And shilly Kife to itpriving thered,\n",
            "Tarmelilat uney his dits mernowk we spirf he.\n",
            "That tim thou, seavens his woulds will apofe,\n",
            "When my stim; for’d then eyersugon, some like his frieth let’th true\n",
            " soverars be?\n",
            "\n",
            "POULIGA.\n",
            "Ho faphe, Antime dowald; yee, sho; who than workis if than wory?\n",
            "\n",
            "Enter Lettle his G\n",
            "5940/5940 [==============================] - 1480s 249ms/step - loss: 1.6471\n",
            "Epoch 6/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.6065\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"der! murder!\n",
            "What may you be? Are you of good or evil?\n",
            "\n",
            "LODOVICO.\n",
            "\"\n",
            "der! murder!\n",
            "What may you be? Are you of good or evil?\n",
            "\n",
            "LODOVICO.\n",
            "So expoke of that ever too abongs that that\n",
            "ald br this perequinch and concee:\n",
            "There beticely; art mights,\n",
            "Thy lirgl for me makes in bilkeed,\n",
            "\n",
            "\n",
            "[Enter conan on alateres is Antind mingre. I rook.\n",
            "\n",
            "\n",
            "\n",
            "ERIIA.\n",
            "What I will sim, my affrostuy.\n",
            "\n",
            "PABIITUS.\n",
            "\n",
            "\n",
            "\n",
            "No, love in this same, away ongestly:\n",
            "“I sequait, that you’\n",
            "5940/5940 [==============================] - 1469s 247ms/step - loss: 1.6065\n",
            "Epoch 7/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.5739\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"ech you, what manner of man is he?\n",
            "\n",
            "FABIAN.\n",
            "Nothing of that wonderf\"\n",
            "ech you, what manner of man is he?\n",
            "\n",
            "FABIAN.\n",
            "\n",
            "\n",
            "BANCLES.\n",
            "O Luck’d donie with heavtl trumen. That becomforp:\n",
            "If if youns thinds I’ll nomen atwhills batemen of this revell is in:\n",
            "Thus buct shall conder hour\n",
            "\n",
            "  Gid to his skees me is breath. And the keeph thouse.\n",
            "\n",
            "\n",
            "    Noon the would reising doth how befo\n",
            "5940/5940 [==============================] - 1479s 249ms/step - loss: 1.5739\n",
            "Epoch 8/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.5465\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \" it manifest where she has liv’d,\n",
            "Or how stol’n from the dead.\n",
            "\n",
            "PAU\"\n",
            " it manifest where she has liv’d,\n",
            "Or how stol’n from the dead.\n",
            "\n",
            "PAUNIUS.\n",
            "I did sone the dush’s his broend world?\n",
            "\n",
            "UTHAch.\n",
            "\n",
            "\n",
            "And new.\n",
            "\n",
            "TROIIS.\n",
            "Thoughts of a gapt bosidiling is lars’ow; then noth you. If it lief\n",
            "comboneds beart’d, an give: a  both smeal host then this speed,\n",
            "Panin’t wition prom’stions hip wornted dead?\n",
            "\n",
            "AUTOBY.\n",
            "Where dreas’s my work, he lovid sirrish?\n",
            "\n",
            "\n",
            "Yearmertys, owe served me sut see in thoughts.\n",
            "\n",
            "CRETIIN. The\n",
            "5940/5940 [==============================] - 1478s 249ms/step - loss: 1.5465\n",
            "Epoch 9/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.5234\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"ht;\n",
            "Now she adds honours to his hateful name.\n",
            "  She clepes him king \"\n",
            "ht;\n",
            "Now she adds honours to his hateful name.\n",
            "  She clepes him king and some stamp, fells\n",
            "\n",
            "\n",
            "      Are you apter'd for hander your despruds,\n",
            "    For Seport. He shark stainst speak's acksorns.\n",
            "  LUCIUS. 1 and in have yet! Go-down and so to but takes never\n",
            "      'th' prave us sorrished and neight stare thumeluze\n",
            "    Will we pisel'd mone all thee dan death fit?\n",
            " \n",
            "5940/5940 [==============================] - 1459s 246ms/step - loss: 1.5234\n",
            "Epoch 10/10\n",
            "5940/5940 [==============================] - ETA: 0s - loss: 1.5039\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"arm blood like wash, and makes his trough\n",
            "    In your embowell'd boso\"\n",
            "arm blood like wash, and makes his trough\n",
            "    In your embowell'd boson,\n",
            "    Make o' this conirliful fearicity image!\n",
            "    For they tear, my place the wife stigh my daEd.\n",
            "  DUKE. Too skithous of extempinigy wish depiding munder,\n",
            "\n",
            "    To gies well he meat of me.\n",
            "  SPAVENTIO. This lave blaine, Sir. Sweet not in chints,\n",
            "\n",
            "\n",
            "“Lot the Enoldiness arverut, bod undo?\n",
            "\n",
            "THALL\n",
            "5940/5940 [==============================] - 1494s 251ms/step - loss: 1.5039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9714277668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeVXlCXOn4WJ",
        "colab_type": "text"
      },
      "source": [
        "###Attempt 2:  Use Tensor Dataset for TPU Acceleration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97x99IsUmN_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming data\n",
        "import tensorflow as tf\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices(x)\n",
        "target =  tf.data.Dataset.from_tensor_slices(y)\n",
        "\n",
        "dataset = tf.data.Dataset.zip((data, target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ07LBpkMpSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate batches\n",
        "\n",
        "batched_Dataset = dataset.batch(256, drop_remainder = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhON6cNTEQe5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-OA0X_o_F3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "05c2c19a-830a-4e84-bbcd-c5e7ed5b2cad"
      },
      "source": [
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plLU60-Ko-4I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd6604d4-db78-4032-953b-50d6cf298ed4"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "  model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  model.fit(batched_Dataset,\n",
        "            steps_per_epoch = x.shape[0]//256,\n",
        "            epochs=10,\n",
        "            callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   1/1100 [..............................] - ETA: 2s - loss: 4.6683WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0081s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0081s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1099/1100 [============================>.] - ETA: 0s - loss: 2.7521\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"y father, and may do any thing.\n",
            "\n",
            "FALSTAFF.\n",
            "Rob me the exchequer the\"\n",
            "y father, and may do any thing.\n",
            "\n",
            "FALSTAFF.\n",
            "Rob me the exchequer theWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "DBe Kerenz quWk, poer tamese mot yaithae, \n",
            "hf serd  pras wey e the bome girot allyend.  ingesafcayhein ffope wenthant \n",
            "V\n",
            "\n",
            " teid ande thersan\n",
            "1100/1100 [==============================] - 169s 154ms/step - loss: 2.7523\n",
            "Epoch 2/10\n",
            "1098/1100 [============================>.] - ETA: 0s - loss: 2.2376\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"lls us rebels, traitors; and will scourge\n",
            "With haughty arms this hate\"\n",
            "lls us rebels, traitors; and will scourge\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Th\n",
            "1100/1100 [==============================] - 169s 154ms/step - loss: 2.2385\n",
            "Epoch 3/10\n",
            "1100/1100 [==============================] - ETA: 0s - loss: 2.0994\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"I’ll find Romeo\n",
            "To comfort you. I wot well where he is.\n",
            "Hark ye, you\"\n",
            "I’ll find Romeo\n",
            "To comfort you. I wot well where he is.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Youle, Gnoch:\n",
            "1100/1100 [==============================] - 169s 154ms/step - loss: 2.0994\n",
            "Epoch 4/10\n",
            "1100/1100 [==============================] - ETA: 0s - loss: 2.0215\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"ndeed, is not under white and black,— this\n",
            "      plaintiff here, the \"\n",
            "ndeed, is not under white and black,— this\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1100/1100 [==============================] - 170s 154ms/step - loss: 2.0215\n",
            "Epoch 5/10\n",
            "1099/1100 [============================>.] - ETA: 0s - loss: 1.9625\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"nd then runs out._]\n",
            "\n",
            "GRATIANO.\n",
            "The woman falls. Sure, he hath kill’\"\n",
            "nd then runs out._]\n",
            "\n",
            "GRATIANO.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Codiry, that buke I'r’d troifinge peneawn will fighef inde\n",
            "1100/1100 [==============================] - 170s 155ms/step - loss: 1.9631\n",
            "Epoch 6/10\n",
            "1098/1100 [============================>.] - ETA: 0s - loss: 1.9119\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"\n",
            "    Too heavy for a man that hopes for heaven!\n",
            "  CROMWELL. I am glad\"\n",
            "\n",
            "    Too heavy for a man that hopes for heaven!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Endef shave redorve ad Ail tnos galn', duy, gomise in hrech foo knes a come with torsh hath nat thit of thou rist me; I \n",
            "1100/1100 [==============================] - 171s 156ms/step - loss: 1.9130\n",
            "Epoch 7/10\n",
            "1099/1100 [============================>.] - ETA: 0s - loss: 1.8725\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"nted?\n",
            "What simple thief brags of his own attaint?\n",
            "’Tis double wrong \"\n",
            "nted?\n",
            "What simple thief brags of his own attaint?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      \n",
            "1100/1100 [==============================] - 170s 154ms/step - loss: 1.8732\n",
            "Epoch 8/10\n",
            "1099/1100 [============================>.] - ETA: 0s - loss: 1.8390\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"s, bid him drop gold, and take it;\n",
            "After he scores, he never pays the\"\n",
            "s, bid him drop gold, and take it;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "What hishade this bo\n",
            "1100/1100 [==============================] - 171s 156ms/step - loss: 1.8396\n",
            "Epoch 9/10\n",
            "1100/1100 [==============================] - ETA: 0s - loss: 1.8115\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"\n",
            "Thank your majesty.\n",
            "\n",
            " [_Exeunt. Flourish._]\n",
            "\n",
            "SCENE III. Rossillon\"\n",
            "\n",
            "Thank your majesty.\n",
            "\n",
            " [_Exeunt. Flourish._]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "And not this undoridod have hast kid remort!\n",
            "1100/1100 [==============================] - 171s 156ms/step - loss: 1.8115\n",
            "Epoch 10/10\n",
            "1097/1100 [============================>.] - ETA: 0s - loss: 1.7835\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"e then,\n",
            "To make me blest or cursed’st among men!\n",
            "\n",
            " [_Cornets. Exeun\"\n",
            "e then,\n",
            "To make me blest or cursed’st among men!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Senel, her-goth, corlu\n",
            "1100/1100 [==============================] - 172s 156ms/step - loss: 1.7853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}